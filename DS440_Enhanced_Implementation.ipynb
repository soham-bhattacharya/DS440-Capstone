{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAbGDvvfl7dN",
        "outputId": "1f6f27c4-c44a-4382-a79d-707e633cc03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.35.1)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install matplotlib imageio gymnasium numpy pytorch stable_baselines3 shimmy pillow os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import math\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "\n",
        "# Load the basketball court image\n",
        "path1 = '/content/basketball_court_half.png'\n",
        "court_img = mpimg.imread(path1)\n",
        "\n",
        "# Define the DQN model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 128)  # state: (x, y)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)  # output Q-values for actions: move, shoot\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Define environment details, actions, and rewards\n",
        "actions = [\"move\", \"shoot\"]\n",
        "gamma = 0.95  # discount factor\n",
        "epsilon = 1.0  # exploration-exploitation tradeoff\n",
        "epsilon_decay = 0.995\n",
        "min_epsilon = 0.01\n",
        "alpha = 0.01  # learning rate\n",
        "\n",
        "# Initialize the DQN model and optimizer\n",
        "model = DQN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=alpha)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Experience replay buffer\n",
        "replay_buffer = deque(maxlen=1000)\n",
        "\n",
        "# Statistics Counters\n",
        "shots_taken = 0\n",
        "shots_made = 0\n",
        "shots_defended = 0\n",
        "ball_lost = 0\n",
        "\n",
        "def distance_from_hoop(x, y):\n",
        "    \"\"\"Calculate the distance from the player to the hoop at (0, 5.2).\"\"\"\n",
        "    return math.sqrt((x ** 2) + ((y - 5.2) ** 2))\n",
        "\n",
        "import math\n",
        "\n",
        "def is_player_between_defender_and_hoop(player_x, player_y, defender_x, defender_y):\n",
        "    \"\"\"\n",
        "    Determine if the defender is positioned between the player and the hoop by checking alignment and boundary.\n",
        "    \"\"\"\n",
        "    hoop_x, hoop_y = 0, 5.2  # Hoop position\n",
        "\n",
        "    # Calculate distances and direction vectors\n",
        "    player_to_hoop_dist = math.sqrt((player_x - hoop_x) ** 2 + (player_y - hoop_y) ** 2)\n",
        "    defender_to_hoop_dist = math.sqrt((defender_x - hoop_x) ** 2 + (defender_y - hoop_y) ** 2)\n",
        "\n",
        "    # Check if defender is closer to hoop than player (first boundary condition)\n",
        "    if defender_to_hoop_dist >= player_to_hoop_dist:\n",
        "        return False\n",
        "\n",
        "    # Calculate the direction vectors\n",
        "    player_vec_x = player_x - hoop_x\n",
        "    player_vec_y = player_y - hoop_y\n",
        "    defender_vec_x = defender_x - hoop_x\n",
        "    defender_vec_y = defender_y - hoop_y\n",
        "\n",
        "    # Normalize direction vectors to check alignment (use cross product)\n",
        "    cross_product = player_vec_x * defender_vec_y - player_vec_y * defender_vec_x\n",
        "    return abs(cross_product) < 1e-5  # Check if vectors are collinear within tolerance\n",
        "\n",
        "def calculate_defender_success(dist):\n",
        "    \"\"\"\n",
        "    Calculate the defender's chance of successfully defending the shot, keeping it generally low.\n",
        "    Probability slightly increases as the player gets closer to the hoop.\n",
        "    \"\"\"\n",
        "    base_defense_prob = 0.01  # Low base defense probability\n",
        "    close_hoop_factor = 0.02 * max(0, (10 - dist) / 10) if dist < 10 else 0\n",
        "    return base_defense_prob + close_hoop_factor\n",
        "\n",
        "def calculate_shot_success(dist):\n",
        "    \"\"\"Calculate shot success probability based on distance, increasing close to the hoop.\"\"\"\n",
        "    # High shot success probability close to the hoop\n",
        "    base_prob = 0.95 if dist < 10 else 0.85 if dist <= 24 else 0.6\n",
        "    scaling_factor = max(1 - (dist / 60), 0.1)  # Scaling for increased success close to the hoop\n",
        "    return max(0.01, base_prob * scaling_factor)\n",
        "\n",
        "def calculate_ball_loss_probability(dist):\n",
        "    \"\"\"Probability of losing the ball rises slightly close to the hoop.\"\"\"\n",
        "    return min(0.01, 0.02 + (10 - dist) * 0.01 if dist < 10 else 0.02)\n",
        "\n",
        "def reward(state, defender_state):\n",
        "    \"\"\"Simulate reward when a shot is taken or ball is lost.\"\"\"\n",
        "    global shots_taken, shots_made, shots_defended, ball_lost\n",
        "    x, y = state\n",
        "    defender_x, defender_y = defender_state\n",
        "    dist = distance_from_hoop(x, y)\n",
        "    shots_taken += 1\n",
        "\n",
        "    # Ball loss probability check\n",
        "    if np.random.random() < calculate_ball_loss_probability(dist):\n",
        "        ball_lost += 1\n",
        "        return -2, dist, \"Lost Ball\"  # End step if ball is lost\n",
        "\n",
        "    # Calculate shot and defender probabilities\n",
        "    defender_prob = calculate_defender_success(dist)\n",
        "    shot_prob = calculate_shot_success(dist)\n",
        "    final_shot_prob = shot_prob * (1 - defender_prob)\n",
        "\n",
        "    # Determine shot outcome\n",
        "    if np.random.random() < final_shot_prob:\n",
        "        shots_made += 1\n",
        "        return (3 if dist > 24 else 2), dist, \"Made Shot\"  # Points for successful shots\n",
        "    else:\n",
        "        shots_defended += 1\n",
        "        return -2, dist, \"Missed Shot\"\n",
        "\n",
        "def select_action(state):\n",
        "    \"\"\"Epsilon-greedy action selection, with shoot ending a step.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(actions)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = model(state_tensor)\n",
        "            return actions[torch.argmax(q_values).item()]\n",
        "\n",
        "def train_model(batch_size=32):\n",
        "    \"\"\"Train the DQN model using experience replay.\"\"\"\n",
        "    if len(replay_buffer) < batch_size:\n",
        "        return\n",
        "    batch = np.random.choice(len(replay_buffer), batch_size, replace=False)\n",
        "    for idx in batch:\n",
        "        state, action, reward_value, next_state = replay_buffer[idx]\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "        target = reward_value + gamma * torch.max(model(next_state_tensor))\n",
        "        q_value = model(state_tensor)[0, actions.index(action)]\n",
        "        loss = loss_fn(q_value, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def clamp(value, min_value, max_value):\n",
        "    \"\"\"Clamp the value to be between min_value and max_value.\"\"\"\n",
        "    return max(min_value, min(value, max_value))\n",
        "\n",
        "def defender_position(player_x, player_y):\n",
        "    \"\"\"Position the defender near the player, but not too close.\"\"\"\n",
        "    offset_x = np.random.randint(-3, 3)\n",
        "    offset_y = np.random.randint(-3, 3)\n",
        "    return clamp(player_x + offset_x, -25, 25), clamp(player_y + offset_y, 0, 47)\n",
        "\n",
        "def visualize_training(state, defender_state, step, total_points, shot_distance, shot_result):\n",
        "    \"\"\"Display player and defender on the court, logging shot distance and outcome.\"\"\"\n",
        "    x, y = state\n",
        "    defender_x, defender_y = defender_state\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(court_img, extent=[-25, 25, 0, 47])\n",
        "\n",
        "    # Plot player, defender, and hoop\n",
        "    plt.scatter(x, y, color='blue', s=200, marker='o', edgecolors='black', label='Player')\n",
        "    plt.scatter(0, 5.2, color='orange', s=100, marker='o', edgecolors='black', label='Hoop')\n",
        "    plt.scatter(defender_x, defender_y, color='red', s=200, marker='o', edgecolors='black', label='Defender')\n",
        "\n",
        "    # Set axis limits and display shot stats\n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(0, 47)\n",
        "    plt.title(f\"Step {step + 1}: Points: {total_points}, Shot Distance: {shot_distance:.2f}ft, \"\n",
        "              f\"Shot Result: {shot_result}\\n\"\n",
        "              f\"Shots Taken: {shots_taken}, Shots Made: {shots_made}, Shots Defended: {shots_defended}, Ball Lost: {ball_lost}\")\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    # Save frame for video\n",
        "    plt.savefig(f'frame_{step}.png')\n",
        "    plt.close()\n",
        "\n",
        "# Simulate training where each step ends only with a shot or ball loss\n",
        "state = (-20, 20)\n",
        "total_points = 0\n",
        "steps = 100\n",
        "for step in range(steps):\n",
        "    while True:\n",
        "        action = select_action(state)\n",
        "        defender_state = defender_position(state[0], state[1])\n",
        "\n",
        "        # Move player for \"move\" action\n",
        "        if action == \"move\":\n",
        "            new_x = clamp(state[0] + np.random.randint(-5, 5), -25, 25)\n",
        "            new_y = clamp(state[1] + np.random.randint(-5, 5), 0, 47)\n",
        "            state = (new_x, new_y)\n",
        "        else:  # \"shoot\" action\n",
        "            reward_value, shot_distance, shot_result = reward(state, defender_state)\n",
        "            total_points += max(0, reward_value)  # Only add positive points\n",
        "            visualize_training(state, defender_state, step, total_points, shot_distance, shot_result)\n",
        "            replay_buffer.append((state, action, reward_value, state))  # Log experience in replay buffer\n",
        "            break  # End of step after shot or ball loss\n",
        "\n",
        "    if step % 5 == 0:\n",
        "        train_model()\n",
        "\n",
        "    # Decay epsilon to transition from exploration to exploitation\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "# Ensure consistent frame sizes for video output\n",
        "frame = cv2.imread('frame_0.png')\n",
        "frame_height, frame_width, _ = frame.shape\n",
        "out = cv2.VideoWriter('training_timelapse.mp4', cv2.VideoWriter_fourcc(*'XVID'), 1, (frame_width, frame_height))\n",
        "\n",
        "# Compile frames into the video\n",
        "for step in range(steps):\n",
        "    frame_path = f'frame_{step}.png'\n",
        "    if os.path.exists(frame_path):\n",
        "        img = cv2.imread(frame_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (frame_width, frame_height))\n",
        "            out.write(img)\n",
        "    else:\n",
        "        print(f\"Frame {step} is missing. Skipping this frame.\")\n",
        "\n",
        "# Release the video writer properly\n",
        "out.release()\n",
        "print(\"Training time-lapse video saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqGtqZN13nZa",
        "outputId": "d6172c80-c607-4088-8518-3dd264da1ee7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time-lapse video saved successfully.\n"
          ]
        }
      ]
    }
  ]
}